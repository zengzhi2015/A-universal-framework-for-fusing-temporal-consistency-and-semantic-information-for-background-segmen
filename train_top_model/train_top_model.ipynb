{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import cv2\n",
    "from matplotlib import pyplot as plt\n",
    "import os\n",
    "from sklearn.utils import shuffle\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prerequisites\n",
    "number_of_logits = 13\n",
    "\n",
    "class Top_Model():\n",
    "\n",
    "    def __init__(self):\n",
    "        self.graph = tf.Graph()\n",
    "        self.sess = tf.Session(graph=self.graph)\n",
    "        with self.graph.as_default():\n",
    "            # Inputs Batch,H,W,C\n",
    "            self.input_probability = tf.placeholder(tf.float32,shape=[None,None,None,1],\n",
    "                                                    name='input_probability')\n",
    "            self.input_logits = tf.placeholder(tf.float32,shape=[None,None,None,number_of_logits],\n",
    "                                                name='input_logits') # A concatination of probability and logits\n",
    "            self.input_positive_mask = tf.placeholder(tf.float32,shape=[None,None,None,1],\n",
    "                                                      name='input_positive_mask') # Used to calculate Fscore*\n",
    "            self.input_negative_mask = tf.placeholder(tf.float32,shape=[None,None,None,1],\n",
    "                                                      name='input_negative_mask')\n",
    "            # Model\n",
    "            self.probability_stack = tf.concat(values=[self.input_probability for t in range(number_of_logits)],\n",
    "                                                     axis=-1,\n",
    "                                                     name='probability_stack')\n",
    "            self.inverse_probability_stack = tf.subtract(1.0,self.probability_stack,\n",
    "                                                         name='inverse_probability_stack')\n",
    "            \n",
    "            self.joint_probabilitys_0 = tf.multiply(self.probability_stack,\n",
    "                                                    self.input_logits,\n",
    "                                                    name='joint_probabilitys_0')\n",
    "            self.joint_probabilitys_1 = tf.multiply(self.inverse_probability_stack,\n",
    "                                                    self.input_logits,\n",
    "                                                    name='joint_probabilitys_1')\n",
    "            \n",
    "            self.conditonal_probabilitys_0 = tf.get_variable('conditonal_probabilitys_0',\n",
    "                                                             shape=[number_of_logits, 1],\n",
    "                                                             initializer=tf.constant_initializer(0.5),\n",
    "                                                             trainable=True,\n",
    "                                                             constraint=lambda t: tf.clip_by_value(t, 0.0, 1.0))\n",
    "            self.conditonal_probabilitys_1 = tf.get_variable('conditonal_probabilitys_1',\n",
    "                                                             shape=[number_of_logits, 1],\n",
    "                                                             initializer=tf.constant_initializer(0.5),\n",
    "                                                             trainable=True,\n",
    "                                                             constraint=lambda t: tf.clip_by_value(t, 0.0, 1.0))\n",
    "\n",
    "            self.merged_result = tf.add(tf.tensordot(self.joint_probabilitys_0, \n",
    "                                                     self.conditonal_probabilitys_0, \n",
    "                                                     axes=[[3], [0]]),\n",
    "                                        tf.tensordot(self.joint_probabilitys_1, \n",
    "                                                     self.conditonal_probabilitys_1, \n",
    "                                                     axes=[[3], [0]]), \n",
    "                                        name='merged_result')\n",
    "\n",
    "            self.TP = tf.reduce_sum(self.input_positive_mask*(1.0-self.merged_result),name='TP')\n",
    "            self.TN = tf.reduce_sum(self.input_negative_mask*self.merged_result,name='TN')\n",
    "            self.FP = tf.reduce_sum(self.input_negative_mask*(1.0-self.merged_result),name='FP')\n",
    "            self.FN = tf.reduce_sum(self.input_positive_mask*self.merged_result,name='FN')\n",
    "\n",
    "            self.Recall = tf.divide(tf.maximum(1e-3,self.TP),\n",
    "                                    tf.maximum(1e-3,self.TP + self.FN),\n",
    "                                    name='Recall')\n",
    "            self.Specificity = tf.divide(tf.maximum(1e-3,self.TN),\n",
    "                                         tf.maximum(1e-3,self.TN + self.FP),\n",
    "                                         name='Specificity')\n",
    "            self.PWC = tf.divide(100.0 * tf.maximum(1e-3,self.FN + self.FP), \n",
    "                                 tf.maximum(1e-3,self.TP + self.FN + self.FP + self.TN),\n",
    "                                 name='PWC')\n",
    "            self.Precision = tf.divide(tf.maximum(1e-3,self.TP),\n",
    "                                       tf.maximum(1e-3,self.TP + self.FP),\n",
    "                                       name='Precision')\n",
    "            self.F_Measure = tf.divide((2 * self.Precision * self.Recall),\n",
    "                                       (self.Precision + self.Recall),\n",
    "                                       name='F_Measure')\n",
    "\n",
    "            self.loss = tf.subtract(1.0,self.F_Measure,name='loss')\n",
    "            \n",
    "            self.optimizer = tf.train.AdamOptimizer(0.01).minimize(self.loss, \n",
    "                                      var_list=[var for var in tf.trainable_variables()]) # original 0.0001\n",
    "\n",
    "            # Log related\n",
    "            self.variable_summaries(self.merged_result,name_scope='merged_result_summary')\n",
    "            self.variable_summaries(self.TP,name_scope='TP_summary')\n",
    "            self.variable_summaries(self.TN,name_scope='TN_summary')\n",
    "            self.variable_summaries(self.FP,name_scope='FP_summary')\n",
    "            self.variable_summaries(self.FN,name_scope='FN_summary')\n",
    "            self.variable_summaries(self.Recall,name_scope='Recall_summary')\n",
    "            self.variable_summaries(self.Specificity,name_scope='Specificity_summary')\n",
    "            self.variable_summaries(self.PWC,name_scope='PWC_summary')\n",
    "            self.variable_summaries(self.Precision,name_scope='Precision_summary')\n",
    "            self.variable_summaries(self.F_Measure,name_scope='F_Measure_summary')\n",
    "            self.variable_summaries(self.loss,name_scope='loss_summary')\n",
    "            self.merged_summary = tf.summary.merge_all()\n",
    "            self.saver = tf.train.Saver(max_to_keep=10000000)\n",
    "            self.train_writer = None\n",
    "            \n",
    "    \n",
    "    def variable_summaries(self,var,name_scope=None):\n",
    "        \"\"\"Attach a lot of summaries to a Tensor (for TensorBoard visualization).\"\"\"\n",
    "        with tf.name_scope(name_scope):\n",
    "            if len(var.shape)==0:\n",
    "                tf.summary.scalar('value', var)\n",
    "            if len(var.shape)>0:\n",
    "                mean = tf.reduce_mean(var)\n",
    "                tf.summary.scalar('mean', mean)\n",
    "                stddev = tf.sqrt(tf.reduce_mean(tf.square(var - mean)))\n",
    "                tf.summary.scalar('stddev', stddev)\n",
    "                tf.summary.scalar('max', tf.reduce_max(var))\n",
    "                tf.summary.scalar('min', tf.reduce_min(var))\n",
    "                tf.summary.histogram('histogram', var)\n",
    "            if len(var.shape)==4 and (var.shape[-1]==1 or var.shape[-1]==3):\n",
    "                tf.summary.image('image',var)\n",
    "    \n",
    "    # Initialization\n",
    "    def initialize_sess(self,log_path):\n",
    "        with self.graph.as_default():\n",
    "            self.sess.run(tf.global_variables_initializer())\n",
    "            if self.train_writer is not None:\n",
    "                self.train_writer.close()\n",
    "            self.train_writer = tf.summary.FileWriter(log_path, self.sess.graph)\n",
    "    \n",
    "    # Model training\n",
    "    def train(self,probability,logits,positive_mask,negative_mask,step=None):\n",
    "        summary,_,loss,result = self.sess.run([self.merged_summary,self.optimizer,self.loss,self.merged_result],\n",
    "                                              feed_dict={self.input_probability:probability, \n",
    "                                                         self.input_logits:logits,\n",
    "                                                         self.input_positive_mask: positive_mask,\n",
    "                                                         self.input_negative_mask: negative_mask})\n",
    "        self.train_writer.add_summary(summary, global_step = step)\n",
    "        \n",
    "        return loss,result\n",
    "        \n",
    "    # Model estimating\n",
    "    def estimate(self,probability,logits):\n",
    "        result = self.sess.run([self.merged_result],\n",
    "                               feed_dict={self.input_probability:probability, \n",
    "                                          self.input_logits:logits})\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    def save_model(self,model_dir):\n",
    "        with self.graph.as_default():\n",
    "            self.saver.save(self.sess,model_dir + '/model.ckpt')\n",
    "    \n",
    "    def load_model(self,model_path):\n",
    "        with self.graph.as_default():\n",
    "            self.saver.restore(self.sess, tf.train.latest_checkpoint(model_path))\n",
    "    \n",
    "    def __del__(self):\n",
    "        self.sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_model = Top_Model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.Variable 'conditonal_probabilitys_0:0' shape=(13, 1) dtype=float32_ref>\n",
      "<tf.Variable 'conditonal_probabilitys_1:0' shape=(13, 1) dtype=float32_ref>\n"
     ]
    }
   ],
   "source": [
    "with top_model.graph.as_default():\n",
    "    for var in tf.trainable_variables():\n",
    "        print(var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_model.initialize_sess(log_path='./Logs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_double_mask(ground_truth):\n",
    "    \"\"\"Calculate the weight for calculating the loss of the model\"\"\"\n",
    "   \n",
    "    positive_mask = np.zeros_like(np.float32(ground_truth))\n",
    "    negative_mask = np.zeros_like(np.float32(ground_truth))\n",
    "\n",
    "    index_object = np.where(ground_truth==255)\n",
    "    index_background = np.where(ground_truth==0)\n",
    "    \n",
    "    positive_mask[index_object] = 1.0\n",
    "    negative_mask[index_background] = 1.0\n",
    "    \n",
    "    return positive_mask, negative_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "PFL = np.load(os.getcwd()+'/File_lists/'+'probability_file_list_for_training_top_model.npy')\n",
    "LFL = np.load(os.getcwd()+'/File_lists/'+'logits_files_list_for_training_top_model.npy')\n",
    "MFL = np.load(os.getcwd()+'/File_lists/'+'mask_file_list_for_training_top_model.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(600,)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PFL.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "temp_count:  1 current_loss: 0.89408636\n",
      "[[0.49000013 0.5099998  0.49000108 0.49000034 0.5099995  0.49002805\n",
      "  0.50999904 0.5099998  0.50999993 0.5099998  0.50999886 0.5099994\n",
      "  0.5099999 ]\n",
      " [0.49000004 0.50999635 0.4900004  0.49000007 0.5099757  0.49000883\n",
      "  0.49000022 0.4900025  0.5099921  0.49001113 0.5099866  0.49001566\n",
      "  0.49000144]]\n",
      "temp_count:  5 current_loss: 0.8872377\n",
      "[[0.45143116 0.54860085 0.4527328  0.45179358 0.5444858  0.4843686\n",
      "  0.53824025 0.54905075 0.5497571  0.5496631  0.5479648  0.547855\n",
      "  0.5493887 ]\n",
      " [0.45104128 0.5472241  0.4513434  0.4518783  0.5171996  0.4507078\n",
      "  0.4520732  0.45371374 0.5422678  0.493345   0.54498535 0.4566083\n",
      "  0.45058608]]\n",
      "temp_count:  9 current_loss: 0.85242903\n",
      "[[0.41230884 0.587758   0.42291126 0.4151195  0.58005905 0.49932364\n",
      "  0.57398134 0.5874822  0.5896088  0.59005713 0.5871304  0.5856751\n",
      "  0.58883286]\n",
      " [0.4110542  0.5852109  0.41358608 0.41452754 0.5046053  0.41410264\n",
      "  0.41368008 0.42063567 0.5517966  0.50349414 0.58282596 0.43108615\n",
      "  0.41212443]]\n",
      "temp_count:  13 current_loss: 0.84268653\n",
      "[[0.37228677 0.6279122  0.39378452 0.3793882  0.61660963 0.52824706\n",
      "  0.6069437  0.6276219  0.6302404  0.63158196 0.6257635  0.62457204\n",
      "  0.62923473]\n",
      " [0.37060466 0.62452686 0.3752305  0.37808973 0.49402687 0.37782133\n",
      "  0.37360707 0.38806972 0.57659584 0.5264206  0.6196705  0.40276426\n",
      "  0.37500995]]\n",
      "temp_count:  17 current_loss: 0.8153076\n",
      "[[0.3320498  0.6696598  0.36365286 0.34297943 0.6544089  0.5627377\n",
      "  0.6421904  0.6689027  0.67241454 0.67429775 0.6657209  0.6648392\n",
      "  0.6704652 ]\n",
      " [0.33017564 0.66590834 0.33615738 0.34056553 0.48252973 0.34236708\n",
      "  0.3324361  0.3549501  0.6098505  0.56076205 0.65829206 0.37173724\n",
      "  0.33982164]]\n",
      "temp_count:  21 current_loss: 0.7753597\n",
      "[[0.2923146  0.71101695 0.3363871  0.3044016  0.6948843  0.5849368\n",
      "  0.6806977  0.7114629  0.71668696 0.718839   0.7080688  0.70670354\n",
      "  0.7131603 ]\n",
      " [0.29005423 0.7066064  0.29651168 0.301134   0.47125196 0.30453205\n",
      "  0.29193506 0.33042932 0.64449    0.6013213  0.69935566 0.3478283\n",
      "  0.30382958]]\n",
      "temp_count:  25 current_loss: 0.7673219\n",
      "[[0.25331843 0.75475204 0.30901197 0.26511887 0.73797786 0.6141962\n",
      "  0.72447413 0.75558484 0.762854   0.76487684 0.7538893  0.7497891\n",
      "  0.7577468 ]\n",
      " [0.24981225 0.74989384 0.2550324  0.2591694  0.46713397 0.26938638\n",
      "  0.25300992 0.31592137 0.6846824  0.6464912  0.7442126  0.32843068\n",
      "  0.26751792]]\n",
      "temp_count:  29 current_loss: 0.6721855\n",
      "[[0.21205707 0.80127674 0.28036278 0.2266359  0.782942   0.6445422\n",
      "  0.770909   0.8028288  0.8107172  0.81301576 0.80147606 0.79518485\n",
      "  0.80523175]\n",
      " [0.2076597  0.795733   0.21219379 0.21779162 0.46995223 0.2339459\n",
      "  0.21187828 0.3170192  0.7300716  0.69447947 0.7911504  0.30352646\n",
      "  0.23348634]]\n",
      "temp_count:  33 current_loss: 0.6345991\n",
      "[[0.16925375 0.8509061  0.26397175 0.19113177 0.8320619  0.684109\n",
      "  0.81521696 0.85316575 0.86123455 0.86336595 0.848285   0.8442318\n",
      "  0.8559763 ]\n",
      " [0.16341189 0.84439856 0.16901322 0.17549682 0.46811032 0.19511047\n",
      "  0.16915733 0.34091878 0.77874917 0.7458784  0.8367471  0.28268957\n",
      "  0.20780754]]\n",
      "temp_count:  37 current_loss: 0.5854866\n",
      "[[0.12637943 0.9018417  0.27108032 0.15604952 0.88384855 0.7295385\n",
      "  0.85935265 0.9048706  0.9151548  0.9168218  0.8964302  0.8966903\n",
      "  0.90869796]\n",
      " [0.1184414  0.89406693 0.1258076  0.1312554  0.47272864 0.15116537\n",
      "  0.12517844 0.37648144 0.83182186 0.80002725 0.8818396  0.27441552\n",
      "  0.19235857]]\n",
      "temp_count:  41 current_loss: 0.44733167\n",
      "[[0.08223805 0.95510596 0.28243655 0.12961622 0.9339905  0.7754445\n",
      "  0.9104714  0.9602278  0.97187465 0.97243714 0.9498221  0.95032907\n",
      "  0.9645873 ]\n",
      " [0.07204433 0.9465475  0.07905395 0.08494511 0.4844825  0.11081068\n",
      "  0.08224031 0.42405948 0.8880118  0.8564512  0.93139225 0.27962482\n",
      "  0.19960356]]\n",
      "temp_count:  45 current_loss: 0.3133527\n",
      "[[0.03910626 1.         0.31487098 0.12676947 0.9890555  0.8230401\n",
      "  0.9661002  1.         1.         1.         1.         1.\n",
      "  1.        ]\n",
      " [0.02337884 1.         0.03297712 0.03806684 0.5091293  0.0707903\n",
      "  0.03771075 0.47612813 0.9464259  0.9152486  0.98434365 0.30007356\n",
      "  0.2331754 ]]\n",
      "temp_count:  49 current_loss: 0.29482037\n",
      "[[0.         1.         0.36366084 0.14749011 1.         0.8755939\n",
      "  1.         1.         1.         1.         1.         1.\n",
      "  1.        ]\n",
      " [0.         1.         0.         0.         0.549462   0.03212858\n",
      "  0.         0.53450125 1.         0.9746722  1.         0.33809507\n",
      "  0.28386474]]\n",
      "temp_count:  53 current_loss: 0.28318667\n",
      "[[0.         1.         0.41547036 0.17971414 1.         0.93094194\n",
      "  1.         1.         1.         1.         1.         1.\n",
      "  1.        ]\n",
      " [0.         1.         0.         0.         0.5894944  0.\n",
      "  0.         0.59461933 1.         1.         1.         0.38533625\n",
      "  0.34009033]]\n",
      "temp_count:  57 current_loss: 0.2843122\n",
      "[[0.         1.         0.46951172 0.21857426 1.         0.988704\n",
      "  1.         1.         1.         1.         1.         1.\n",
      "  1.        ]\n",
      " [0.         1.         0.         0.         0.6354825  0.\n",
      "  0.         0.65735763 1.         1.         1.         0.4329217\n",
      "  0.40099597]]\n",
      "temp_count:  61 current_loss: 0.22403002\n",
      "[[0.         1.         0.5272929  0.26364672 1.         1.\n",
      "  1.         1.         1.         1.         1.         1.\n",
      "  1.        ]\n",
      " [0.         1.         0.         0.         0.6885381  0.\n",
      "  0.         0.7185881  1.         1.         1.         0.48622435\n",
      "  0.46203652]]\n",
      "temp_count:  65 current_loss: 0.25472426\n",
      "[[0.         1.         0.58122987 0.3109654  1.         1.\n",
      "  1.         1.         1.         1.         1.         1.\n",
      "  1.        ]\n",
      " [0.         1.         0.         0.         0.7334911  0.\n",
      "  0.         0.7779942  1.         1.         1.         0.537558\n",
      "  0.5214235 ]]\n",
      "temp_count:  69 current_loss: 0.24669707\n",
      "[[0.         1.         0.6341341  0.35918874 1.         1.\n",
      "  1.         1.         1.         1.         1.         1.\n",
      "  1.        ]\n",
      " [0.         1.         0.         0.         0.7802525  0.\n",
      "  0.         0.8386374  1.         1.         1.         0.59193057\n",
      "  0.58192444]]\n",
      "temp_count:  73 current_loss: 0.22729802\n",
      "[[0.         1.         0.6835042  0.4110487  1.         1.\n",
      "  1.         1.         1.         1.         1.         1.\n",
      "  1.        ]\n",
      " [0.         1.         0.         0.         0.8251783  0.\n",
      "  0.         0.89449215 1.         1.         1.         0.64666164\n",
      "  0.63850635]]\n",
      "temp_count:  77 current_loss: 0.24384654\n",
      "[[0.         1.         0.7293052  0.46210682 1.         1.\n",
      "  1.         1.         1.         1.         1.         1.\n",
      "  1.        ]\n",
      " [0.         1.         0.         0.         0.86434233 0.\n",
      "  0.         0.95189106 1.         1.         1.         0.6992124\n",
      "  0.69605565]]\n",
      "temp_count:  81 current_loss: 0.2467342\n",
      "[[0.         1.         0.7777338  0.5142859  1.         1.\n",
      "  1.         1.         1.         1.         1.         1.\n",
      "  1.        ]\n",
      " [0.         1.         0.         0.         0.9055648  0.\n",
      "  0.         1.         1.         1.         1.         0.751654\n",
      "  0.75335777]]\n",
      "temp_count:  85 current_loss: 0.23799288\n",
      "[[0.         1.         0.82640415 0.56879944 1.         1.\n",
      "  1.         1.         1.         1.         1.         1.\n",
      "  1.        ]\n",
      " [0.         1.         0.         0.         0.9465946  0.\n",
      "  0.         1.         1.         1.         1.         0.80243254\n",
      "  0.80779177]]\n",
      "temp_count:  89 current_loss: 0.2501623\n",
      "[[0.         1.         0.8744138  0.62245667 1.         1.\n",
      "  1.         1.         1.         1.         1.         1.\n",
      "  1.        ]\n",
      " [0.         1.         0.         0.         0.9835141  0.\n",
      "  0.         1.         1.         1.         1.         0.85226375\n",
      "  0.86270475]]\n",
      "temp_count:  93 current_loss: 0.24630648\n",
      "[[0.        1.        0.9192743 0.6756841 1.        1.        1.\n",
      "  1.        1.        1.        1.        1.        1.       ]\n",
      " [0.        1.        0.        0.        1.        0.        0.\n",
      "  1.        1.        1.        1.        0.9020877 0.9163483]]\n",
      "temp_count:  97 current_loss: 0.24887091\n",
      "[[0.         1.         0.9692733  0.72768307 1.         1.\n",
      "  1.         1.         1.         1.         1.         1.\n",
      "  1.        ]\n",
      " [0.         1.         0.         0.         1.         0.\n",
      "  0.         1.         1.         1.         1.         0.95342034\n",
      "  0.97076404]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "temp_count:  101 current_loss: 0.2104873\n",
      "[[0.        1.        1.        0.7795317 1.        1.        1.\n",
      "  1.        1.        1.        1.        1.        1.       ]\n",
      " [0.        1.        0.        0.        1.        0.        0.\n",
      "  1.        1.        1.        1.        1.        1.       ]]\n",
      "temp_count:  105 current_loss: 0.19521481\n",
      "[[0.        1.        1.        0.8326674 1.        1.        1.\n",
      "  1.        1.        1.        1.        1.        1.       ]\n",
      " [0.        1.        0.        0.        1.        0.        0.\n",
      "  1.        1.        1.        1.        1.        1.       ]]\n",
      "temp_count:  109 current_loss: 0.21020299\n",
      "[[0.         1.         1.         0.88493896 1.         1.\n",
      "  1.         1.         1.         1.         1.         1.\n",
      "  1.        ]\n",
      " [0.         1.         0.         0.         1.         0.\n",
      "  0.         1.         1.         1.         1.         1.\n",
      "  1.        ]]\n",
      "temp_count:  113 current_loss: 0.22867936\n",
      "[[0.         1.         1.         0.93680006 1.         1.\n",
      "  1.         1.         1.         1.         1.         1.\n",
      "  1.        ]\n",
      " [0.         1.         0.         0.         1.         0.\n",
      "  0.         1.         1.         1.         1.         1.\n",
      "  1.        ]]\n",
      "temp_count:  117 current_loss: 0.20268083\n",
      "[[0.        1.        1.        0.9898767 1.        1.        1.\n",
      "  1.        1.        1.        1.        1.        1.       ]\n",
      " [0.        1.        0.        0.        1.        0.        0.\n",
      "  1.        1.        1.        1.        1.        1.       ]]\n",
      "temp_count:  121 current_loss: 0.2255674\n",
      "[[0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [0. 1. 0. 0. 1. 0. 0. 1. 1. 1. 1. 1. 1.]]\n",
      "temp_count:  125 current_loss: 0.18713766\n",
      "[[0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [0. 1. 0. 0. 1. 0. 0. 1. 1. 1. 1. 1. 1.]]\n",
      "temp_count:  129 current_loss: 0.2413714\n",
      "[[0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [0. 1. 0. 0. 1. 0. 0. 1. 1. 1. 1. 1. 1.]]\n",
      "temp_count:  133 current_loss: 0.25608623\n",
      "[[0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [0. 1. 0. 0. 1. 0. 0. 1. 1. 1. 1. 1. 1.]]\n",
      "temp_count:  137 current_loss: 0.18646151\n",
      "[[0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [0. 1. 0. 0. 1. 0. 0. 1. 1. 1. 1. 1. 1.]]\n",
      "temp_count:  141 current_loss: 0.19380575\n",
      "[[0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [0. 1. 0. 0. 1. 0. 0. 1. 1. 1. 1. 1. 1.]]\n",
      "temp_count:  145 current_loss: 0.22962272\n",
      "[[0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [0. 1. 0. 0. 1. 0. 0. 1. 1. 1. 1. 1. 1.]]\n",
      "temp_count:  149 current_loss: 0.21979505\n",
      "[[0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [0. 1. 0. 0. 1. 0. 0. 1. 1. 1. 1. 1. 1.]]\n",
      "temp_count:  153 current_loss: 0.1946997\n",
      "[[0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [0. 1. 0. 0. 1. 0. 0. 1. 1. 1. 1. 1. 1.]]\n",
      "temp_count:  157 current_loss: 0.1935311\n",
      "[[0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [0. 1. 0. 0. 1. 0. 0. 1. 1. 1. 1. 1. 1.]]\n",
      "temp_count:  161 current_loss: 0.19982326\n",
      "[[0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [0. 1. 0. 0. 1. 0. 0. 1. 1. 1. 1. 1. 1.]]\n",
      "temp_count:  165 current_loss: 0.2415716\n",
      "[[0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [0. 1. 0. 0. 1. 0. 0. 1. 1. 1. 1. 1. 1.]]\n",
      "temp_count:  169 current_loss: 0.18667376\n",
      "[[0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [0. 1. 0. 0. 1. 0. 0. 1. 1. 1. 1. 1. 1.]]\n",
      "temp_count:  173 current_loss: 0.19536239\n",
      "[[0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [0. 1. 0. 0. 1. 0. 0. 1. 1. 1. 1. 1. 1.]]\n",
      "temp_count:  177 current_loss: 0.22210121\n",
      "[[0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [0. 1. 0. 0. 1. 0. 0. 1. 1. 1. 1. 1. 1.]]\n",
      "temp_count:  181 current_loss: 0.17325938\n",
      "[[0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [0. 1. 0. 0. 1. 0. 0. 1. 1. 1. 1. 1. 1.]]\n",
      "temp_count:  185 current_loss: 0.24336946\n",
      "[[0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [0. 1. 0. 0. 1. 0. 0. 1. 1. 1. 1. 1. 1.]]\n",
      "temp_count:  189 current_loss: 0.220725\n",
      "[[0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [0. 1. 0. 0. 1. 0. 0. 1. 1. 1. 1. 1. 1.]]\n",
      "temp_count:  193 current_loss: 0.22290635\n",
      "[[0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [0. 1. 0. 0. 1. 0. 0. 1. 1. 1. 1. 1. 1.]]\n",
      "temp_count:  197 current_loss: 0.15604258\n",
      "[[0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [0. 1. 0. 0. 1. 0. 0. 1. 1. 1. 1. 1. 1.]]\n",
      "temp_count:  201 current_loss: 0.19606662\n",
      "[[0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [0. 1. 0. 0. 1. 0. 0. 1. 1. 1. 1. 1. 1.]]\n",
      "temp_count:  205 current_loss: 0.21040654\n",
      "[[0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [0. 1. 0. 0. 1. 0. 0. 1. 1. 1. 1. 1. 1.]]\n",
      "temp_count:  209 current_loss: 0.2426604\n",
      "[[0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [0. 1. 0. 0. 1. 0. 0. 1. 1. 1. 1. 1. 1.]]\n",
      "temp_count:  213 current_loss: 0.21003127\n",
      "[[0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [0. 1. 0. 0. 1. 0. 0. 1. 1. 1. 1. 1. 1.]]\n",
      "temp_count:  217 current_loss: 0.16980445\n",
      "[[0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      " [0. 1. 0. 0. 1. 0. 0. 1. 1. 1. 1. 1. 1.]]\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "num_epochs = 20\n",
    "batch_size = 50\n",
    "num_batchs = len(PFL)//batch_size -1\n",
    "\n",
    "H_resize = 240\n",
    "W_resize = 320\n",
    "\n",
    "temp_count = 0\n",
    "for epoch in range(num_epochs):\n",
    "    PFL_shuffled, LFL_shuffled, MFL_shuffled = shuffle(PFL,LFL,MFL,random_state=epoch)\n",
    "    for batch_num in range(num_batchs):\n",
    "        \n",
    "        # Build the learning batch\n",
    "        probability_batch_file_list = PFL_shuffled[batch_num*batch_size:(1+batch_num)*batch_size]\n",
    "        logits_batch_file_list = LFL_shuffled[batch_num*batch_size:(1+batch_num)*batch_size]\n",
    "        mask_batch_file_list = MFL_shuffled[batch_num*batch_size:(1+batch_num)*batch_size]\n",
    "        \n",
    "        probability_batch = []\n",
    "        logits_batch = []\n",
    "        positive_mask_batch = []\n",
    "        negative_mask_batch = []\n",
    "        \n",
    "        for probability_file,logit_files,truth_file in zip(probability_batch_file_list,\n",
    "                                                           logits_batch_file_list,\n",
    "                                                           mask_batch_file_list):\n",
    "            truth = np.asarray(Image.open(os.getcwd()+'/groundtruths/'+truth_file)) #uint8\n",
    "            truth = cv2.resize(truth,(W_resize, H_resize), interpolation = cv2.INTER_NEAREST)\n",
    "            positive_mask, negative_mask = calculate_double_mask(truth) # float32\n",
    "            probability = np.asarray(Image.open(os.getcwd()+'/probabilities/'+probability_file)) #uint8\n",
    "            probability = np.float32(cv2.resize(probability,(W_resize, H_resize), \n",
    "                                                interpolation = cv2.INTER_CUBIC))/255.0 #float32\n",
    "\n",
    "            logits = []\n",
    "            for index_logit,logit_file in enumerate(logit_files):\n",
    "                logit = np.asarray(Image.open(os.getcwd()+'/logits/'+logit_file)) #uint8\n",
    "                logit = np.float32(cv2.resize(logit,(W_resize, H_resize), \n",
    "                                              interpolation = cv2.INTER_CUBIC))/255.0 #float32\n",
    "                logits.append(np.expand_dims(logit,axis=-1))\n",
    "            \n",
    "            probability_batch.append(np.expand_dims(probability,axis=-1))\n",
    "            logits_batch.append(np.concatenate(logits,axis=-1))\n",
    "            positive_mask_batch.append(np.expand_dims(positive_mask,axis=-1))\n",
    "            negative_mask_batch.append(np.expand_dims(negative_mask,axis=-1))\n",
    "\n",
    "        # Learning\n",
    "        current_loss, merged_result = top_model.train(np.array(probability_batch),\n",
    "                                                      np.array(logits_batch),\n",
    "                                                      np.array(positive_mask_batch),\n",
    "                                                      np.array(negative_mask_batch),\n",
    "                                                      step=temp_count)\n",
    "\n",
    "        \n",
    "        cv2.imshow('logit_sample',logits_batch[0][:,:,[0,3,6]])\n",
    "        cv2.imshow('merged_result_sample',merged_result[0,:,:,0])\n",
    "        cv2.imshow('probability_sample',np.array(probability_batch)[0,:,:,0])\n",
    "        cv2.waitKey(1)\n",
    "\n",
    "        temp_count += 1\n",
    "        if temp_count%4==1:\n",
    "            print('temp_count: ',temp_count,'current_loss:',current_loss)\n",
    "            trainable_variable_list = []\n",
    "            trainable_variable_value_list = []\n",
    "            with top_model.graph.as_default():\n",
    "                for var in tf.trainable_variables():\n",
    "                    trainable_variable_list.append(var)\n",
    "                    trainable_variable_value_list.append(top_model.sess.run(var))\n",
    "            print(np.array(trainable_variable_value_list).T[0].T)\n",
    "\n",
    "cv2.destroyAllWindows()\n",
    "top_model.save_model('./Logs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./Logs/model.ckpt\n"
     ]
    }
   ],
   "source": [
    "top_model.load_model(model_path='./Logs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.Variable 'conditonal_probabilitys_0:0' shape=(13, 1) dtype=float32_ref>\n",
      "<tf.Variable 'conditonal_probabilitys_1:0' shape=(13, 1) dtype=float32_ref>\n"
     ]
    }
   ],
   "source": [
    "trainable_variable_list = []\n",
    "trainable_variable_value_list = []\n",
    "with top_model.graph.as_default():\n",
    "    for var in tf.trainable_variables():\n",
    "        trainable_variable_list.append(var)\n",
    "        print(var)\n",
    "        trainable_variable_value_list.append(top_model.sess.run(var).T[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.], dtype=float32),\n",
       " array([0., 1., 0., 0., 1., 1., 0., 1., 1., 1., 1., 1., 1.], dtype=float32)]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainable_variable_value_list"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
